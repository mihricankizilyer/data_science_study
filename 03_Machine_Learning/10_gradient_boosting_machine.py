# -*- coding: utf-8 -*-
"""3.6 Gradient Boosting Machine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OZeeC4pSYlkdFr9zOfnDrtdeITsAM0e

# **GBM**

- AdaBoost'un sınıflandırma ve regresyon problemlerine kolayca uyarlanabilen genelleştirilmiş versiyonudur.

- Artıklar üzerinde tek bir tahminsel model formunda olan modeller serisi kurulur.

Ör: Zayıf öğrenicileri bir araya getirip güçlü bir öğrenici ortaya çıkarmak fikrine dayanır.

**Adaptive Boosting(AdaBoost)**: Zayıf sınırlandırıcıların biraraya gelerek güçlü bir sınıflandırıcı oluşturma fikrini hayata geçiren algoritmadır.

- Gradient boosting tek bir model formunda olan modeller serisi oluşturur.

- Seri içerisindeki bir model serideki bir önceki modelin tahmin artıklarının/ hatalarının üzerine kurularak(fit) oluşturur.

- GBM diferansiyellenebilen herhangi bir kayıp fonksyinuna optimize edebilen Gradient descent algoritmasını kullanmaktadır.

- GB bir çok temel öğrenici tipi (base learner type) kullanabilir.(Trees, linear terms..)

- Cost fonksiyonları ve link fonksiyonları modifiye edilebilir.

- Boosting + Gradient Descent
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn import model_selection
import matplotlib.pyplot as plt
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import LassoCV
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

df = pd.read_csv("/content/Hitters.csv")
df = df.dropna()
dms = pd.get_dummies(df[['League','Division','NewLeague']])
y= df["Salary"]

X_ = df.drop(['Salary','League','Division','NewLeague'], axis = 1).astype('float64')
X = pd.concat([X_, dms[['League_N','Division_W','NewLeague_N']]], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 42)

"""## Model & Tahmin"""

gbm_model = GradientBoostingRegressor().fit(X_train, y_train)

gbm_model

y_pred = gbm_model.predict(X_test)

np.sqrt(mean_squared_error(y_test, y_pred))

"""## Model Tuning"""

gbm_model = GradientBoostingRegressor().fit(X_train, y_train)
gbm_model

"""**criterion='friedman_mse'**
- Bölünmelerle ilgili saflık ölçüsünü ifade eder.

**learning_rate**
- Ağaçların katkısı ile ilgili göz önünde bulundurulan bir değerdir. 

**loss**
- Kayıp Fonksiyonu
- **ls**: en küçük kareleri ifade etmektedir. Ne demek ? -> Gerçek değerler ile tahmin edilen değerler arasındaki farkların karelerinin toplamını min yapmaya çalışılır.

**n_estimators**
- Kullanılacak olan ağaç sayısını ifade eder. 

**subsample**
- Oluşturulacak olan ağaçları oluştururken göz önünde bulundurulacak olan oranı ifade etmektedir. 1 yazılıdğında hepsini oluşturarak ağaç yapmış oluyor.
"""

gbm_params = {"learning_rate": [0.001, 0.1, 0.01],
              "max_depth": [3,5,8],
              "n_estimators": [100, 200,500],
              "subsample": [1,0.5,0.8],
              "loss": ["ls","lad","quantile"]}

gbm_model = GradientBoostingRegressor().fit(X_train, y_train)

gbm_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)

"""- işlemcileri performanlsı bir şekilde kullan -> n_jobs = -1

- ne yapıldığını görmek için ->verbose
"""

gbm_cv_model.best_params_

gbm_tuned = GradientBoostingRegressor(learning_rate= ,
                                      loss = "lad",
                                      max_depth = 3,
                                      n_estimators = 200,
                                      subsample=  ).fit(X_train, y_train)

y_pred = gbm_tuned.predict(X_test)

np.sqrt(mean_squared_error(y_test, y_pred))

#Değişkenlerin önem düzeyi

Importance = pd.DataFrame({'Importance': gbm_tuned.feature_importances_*100},
                          index= X_train.columns)

Importance.sort_values(by = 'Importance',
                       axis = 0,
                       ascending = True).plt(kind = 'barh',
                                             color = 'r')
                       
plt.xlabel('Variable Importance')
plt.gca.legend_ = None