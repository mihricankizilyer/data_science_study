# -*- coding: utf-8 -*-
"""Ridge Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fo0LK_Gnlp_4vJj9_G8sllYbG5acYVoE

# **Ridge Regresyon**

Amaç hata kareler toplamını minimize eden katsayıları, bu katsayılara bir ceza uygulayarak bulmaktır.

- Aşırı öğrenmeye karşı dirençlidir.
- Yanlıdır fakat varyansı düşüktür.
- Çok fazla parametre olduğunda EKK'ya göre daha iyidir.
- Çok boyutluluk lanetine karşı çözüm sunar.
- Çoklu doğrusal bağlantı problemi olduğunda etkilidir.
- Tüm değişkenler ile model kurar. İlgisiz değişkenleri modelden çıkarmaz, katsayılarını sıfıra yaklaştırır.
- Lamda kritik roldedir. İki terimin göreceli etkilerini kontrol etmeyi sağlar.
- Lamda için iyi bir değer bulunması önemlidir. Bunun için CV yöntemi kullanılır.
- Lamdanın sıfır olduğu yer klasik regresyondur. Hata kareler toplamı minimum yapan lamdayı arıyoruz.
- Lamda için belirli değerleri içeren bir küme seçilir ve her birisi için cross validation test hatası hesaplanır.
- En küçük cross validation u veren lamda ayar parametresi olarak seçilir.
- Son olarak seçilen bu lamda ile model yeniden tüm gözlemlere fit edilir.

## **Model**
"""

#Gereli kütüphaneler
import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn import model_selection
import matplotlib.pyplot as plt
from sklearn.linear_model import RidgeCV

#Veri Seti

df = pd.read_csv("/content/Hitters.csv")

df = df.dropna()
#İçerisindeki eksik değerler atılır

dms = pd.get_dummies(df[['League','Division','NewLeague']])
#Veri seti içerisindeki kategorik değişkenleri dummy değişkenine çevirilir.
#Kategorik değişkenlerin sunduğu bilgiyi daha iyi alabilmek adına
#One hot encoding yapılmış oldu.

y= df["Salary"]
#Bağımlı değişken

X_ = df.drop(['Salary','League','Division','NewLeague'], axis = 1).astype('float64')

X = pd.concat([X_, dms[['League_N','Division_W','NewLeague_N']]], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 42)

df.head()

df.shape

# Amaç salary değerini diğer değişkenleri kullanarak tahmin etmeye çalışmak.

ridge_model = Ridge(alpha = 0.1).fit(X_train, y_train)
ridge_model

ridge_model.coef_

ridge_model.intercept_

lambdalar = 10**np.linspace(10,-2,100)*0.5

ridge_model = Ridge()
katsayılar = []

for i in lambdalar:
  ridge_model.set_params(alpha = i)
  ridge_model.fit(X_train, y_train)
  katsayılar.append(ridge_model.coef_)

ax = plt.gca()
ax.plot(lambdalar , katsayılar)
ax.set_xscale("log")

"""Farklı lambda değerlerine karşılık elimizdeki parametrelerin(katsayıların) nasıl değiştiği bilgisi verilir.

## **Tahmin**
"""

ridge_model = Ridge().fit(X_train, y_train)

y_pred = ridge_model.predict(X_train)
# y_pred:tahmin edilen y'ler

y_pred[0:10]

y_train[0:10] #gerçek değerler

RMSE = np.sqrt(mean_squared_error(y_train, y_pred))
RMSE

"""## **Model Tuning**"""

ridge_model = Ridge().fit(X_train, y_train)
y_pred = ridge_model.predict(X_test)
np.sqrt(mean_squared_error(y_test, y_pred))

np.random.randint(0,100,10)

lambdalar1 = np.random.randint(0,1000,100)
lambdalar2 = 10**np.linspace(10,-2,100)*0.5

from sklearn.model_selection import cross_val_score
ridgecv = RidgeCv(alphas = lambdalar2, scoring = "neg_mean_squared_error", cv = 10, normalize = True)
ridgecv.fit(X_train, y_train)

